# -*- coding: utf-8 -*-
"""Split and flexural file name book2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11nkvmruPuqdaTgqnjzbNCmnQ3eVz9GA0
"""

pip install pandas

import pandas as pd

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Display the first few rows of the dataframe
print(df.head())

pip install pandas scikit-learn xgboost

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df['Split tensile Strength', 'Flex']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Calculate R² scores
r2_scores = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)

# Display R² scores
for model_name, r2 in r2_scores.items():
    print(f"{model_name}: R² = {r2:.4f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores
r2_scores = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)

# Display R² scores
for model_name, r2 in r2_scores.items():
    print(f"{model_name}: R² = {r2:.4f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores
r2_scores = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred, multioutput='uniform_average')

# Display R² scores
for model_name, r2 in r2_scores.items():
    print(f"{model_name}: R² = {r2:.4f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true, y_pred)[0, 1]
    alpha = np.std(y_pred) / np.std(y_true)
    beta = np.mean(y_pred) / np.mean(y_true)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate performance metrics
metrics = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')
    mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')
    msle = mean_squared_log_error(y_test, y_pred, multioutput='raw_values')
    r2 = r2_score(y_test, y_pred, multioutput='raw_values')
    medae = median_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    smape_val = smape(y_test, y_pred)
    rae_val = rae(y_test, y_pred)
    kge_val = kge(y_test, y_pred)
    nse_val = nse(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred, multioutput='raw_values')

    metrics[name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'MSLE': msle,
        'R²': r2,
        'MedAE': medae,
        'MAPE': mape,
        'SMAPE': smape_val,
        'RAE': rae_val,
        'KGE': kge_val,
        'NSE': nse_val,
        'EVS': evs
    }

# Display performance metrics
for model_name, metric_values in metrics.items():
    print(f"Model: {model_name}")
    for metric, value in metric_values.items():
        print(f"  {metric}: {value}")
    print()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate performance metrics
metrics = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')
    mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')
    # Ensure no negative values for MSLE
    if (y_test >= 0).all().all() and (y_pred >= 0).all().all():
        msle = mean_squared_log_error(y_test, y_pred, multioutput='raw_values')
    else:
        msle = 'NaN'
    r2 = r2_score(y_test, y_pred, multioutput='raw_values')
    medae = median_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    smape_val = smape(y_test, y_pred)
    rae_val = rae(y_test, y_pred)
    kge_val = kge(y_test, y_pred)
    nse_val = nse(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred, multioutput='raw_values')

    metrics[name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'MSLE': msle,
        'R²': r2,
        'MedAE': medae,
        'MAPE': mape,
        'SMAPE': smape_val,
        'RAE': rae_val,
        'KGE': kge_val,
        'NSE': nse_val,
        'EVS': evs
    }

# Display performance metrics
for model_name, metric_values in metrics.items():
    print(f"Model: {model_name}")
    for metric, value in metric_values.items():
        print(f"  {metric}: {value}")
    print()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate performance metrics
metrics = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')
    mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')
    # Ensure no negative values for MSLE
    if (y_test >= 0).all().all() and (y_pred >= 0).all().all():
        msle = mean_squared_log_error(y_test, y_pred, multioutput='raw_values')
    else:
        msle = 'NaN'
    r2 = r2_score(y_test, y_pred, multioutput='raw_values')
    medae = median_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    smape_val = smape(y_test, y_pred)
    rae_val = rae(y_test, y_pred)
    kge_val = kge(y_test, y_pred)
    nse_val = nse(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred, multioutput='raw_values')

    metrics[name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'MSLE': msle,
        'R²': r2,
        'MedAE': medae,
        'MAPE': mape,
        'SMAPE': smape_val,
        'RAE': rae_val,
        'KGE': kge_val,
        'NSE': nse_val,
        'EVS': evs
    }

# Save the Excel file
output_file = 'model_1performance_metrics.xlsx'
wb.save(output_file)

print(f"Performance metrics saved to {output_file}")

import pandas as pd
import openpyxl
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()

# Create the sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
ws1.append(['Model', 'Type', 'Actual', 'Predicted'])

# Collect actual vs predicted values
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    for i, (actual, pred) in enumerate(zip(y_test.values.T, y_pred.T)):
        if i == 0:
            column_name = 'Split tensile Strength'
        else:
            column_name = 'Flex'

        for actual_value, predicted_value in zip(actual, pred):
            ws1.append([name, column_name, actual_value, predicted_value])

# Save the Excel file
output_file = 'actual_vs_predicted_values.xlsx'
wb.save(output_file)

print(f"Actual vs predicted values saved to {output_file}")

import pandas as pd
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()
ws = wb.active
ws.title = "Model Hyperparameters"

# Write headers
headers = ['Model', 'Hyperparameter', 'Value']
ws.append(headers)

# Function to convert complex values to string
def convert_to_str(value):
    if isinstance(value, (list, tuple, np.ndarray)):
        return str(value)
    return value

# Extract hyperparameters and write to Excel
for name, model in models.items():
    # Extract hyperparameters as a dictionary
    params = model.get_params()

    for param, value in params.items():
        value_str = convert_to_str(value)
        ws.append([name, param, value_str])

# Save the Excel file
output_file = 'model_hyperparameters.xlsx'
wb.save(output_file)

print(f"Model hyperparameters saved to {output_file}")

import pandas as pd

# Convert metrics dictionary to DataFrame
metrics_df = pd.DataFrame(metrics).T

# Define the file path for saving the results
results_file_path = 'model11_performance_metrics.xlsx'

# Save the DataFrame to an Excel file
metrics_df.to_excel(results_file_path, sheet_name='Performance Metrics')

print(f'Results have been saved to {results_file_path}')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate performance metrics
metrics = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')
    mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')
    # Ensure no negative values for MSLE
    if (y_test >= 0).all().all() and (y_pred >= 0).all().all():
        msle = mean_squared_log_error(y_test, y_pred, multioutput='raw_values')
    else:
        msle = 'NaN'
    r2 = r2_score(y_test, y_pred, multioutput='raw_values')
    medae = median_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    smape_val = smape(y_test, y_pred)
    rae_val = rae(y_test, y_pred)
    kge_val = kge(y_test, y_pred)
    nse_val = nse(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred, multioutput='raw_values')

    metrics[name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'MSLE': msle,
        'R²': r2,
        'MedAE': medae,
        'MAPE': mape,
        'SMAPE': smape_val,
        'RAE': rae_val,
        'KGE': kge_val,
        'NSE': nse_val,
        'EVS': evs
    }

# Convert metrics to DataFrame
metrics_df = pd.DataFrame(metrics).T

# Save results to Excel
results_file_path = 'model_12performance_metrics.xlsx'
metrics_df.to_excel(results_file_path, sheet_name='Metrics')

print(f"Metrics saved to {results_file_path}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Initialize metrics dictionaries
metrics_split_tensile = {}
metrics_flexural = {}

# Calculate performance metrics
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Metrics for Split tensile Strength
    rmse_split = np.sqrt(mean_squared_error(y_test['Split tensile Strength'], y_pred[:, 0]))
    mse_split = mean_squared_error(y_test['Split tensile Strength'], y_pred[:, 0])
    mae_split = mean_absolute_error(y_test['Split tensile Strength'], y_pred[:, 0])
    # Ensure no negative values for MSLE
    if (y_test['Split tensile Strength'] >= 0).all() and (y_pred[:, 0] >= 0).all():
        msle_split = mean_squared_log_error(y_test['Split tensile Strength'], y_pred[:, 0])
    else:
        msle_split = 'NaN'
    r2_split = r2_score(y_test['Split tensile Strength'], y_pred[:, 0])
    medae_split = median_absolute_error(y_test['Split tensile Strength'], y_pred[:, 0])
    mape_split = np.mean(np.abs((y_test['Split tensile Strength'] - y_pred[:, 0]) / y_test['Split tensile Strength'])) * 100
    smape_split = smape(y_test['Split tensile Strength'], y_pred[:, 0])
    rae_split = rae(y_test['Split tensile Strength'], y_pred[:, 0])
    kge_split = kge(y_test['Split tensile Strength'], y_pred[:, 0])
    nse_split = nse(y_test['Split tensile Strength'], y_pred[:, 0])
    evs_split = explained_variance_score(y_test['Split tensile Strength'], y_pred[:, 0])

    metrics_split_tensile[name] = {
        'RMSE': rmse_split,
        'MSE': mse_split,
        'MAE': mae_split,
        'MSLE': msle_split,
        'R²': r2_split,
        'MedAE': medae_split,
        'MAPE': mape_split,
        'SMAPE': smape_split,
        'RAE': rae_split,
        'KGE': kge_split,
        'NSE': nse_split,
        'EVS': evs_split
    }

    # Metrics for Flexural Strength
    rmse_flex = np.sqrt(mean_squared_error(y_test['Flex'], y_pred[:, 1]))
    mse_flex = mean_squared_error(y_test['Flex'], y_pred[:, 1])
    mae_flex = mean_absolute_error(y_test['Flex'], y_pred[:, 1])
    # Ensure no negative values for MSLE
    if (y_test['Flex'] >= 0).all() and (y_pred[:, 1] >= 0).all():
        msle_flex = mean_squared_log_error(y_test['Flex'], y_pred[:, 1])
    else:
        msle_flex = 'NaN'
    r2_flex = r2_score(y_test['Flex'], y_pred[:, 1])
    medae_flex = median_absolute_error(y_test['Flex'], y_pred[:, 1])
    mape_flex = np.mean(np.abs((y_test['Flex'] - y_pred[:, 1]) / y_test['Flex'])) * 100
    smape_flex = smape(y_test['Flex'], y_pred[:, 1])
    rae_flex = rae(y_test['Flex'], y_pred[:, 1])
    kge_flex = kge(y_test['Flex'], y_pred[:, 1])
    nse_flex = nse(y_test['Flex'], y_pred[:, 1])
    evs_flex = explained_variance_score(y_test['Flex'], y_pred[:, 1])

    metrics_flexural[name] = {
        'RMSE': rmse_flex,
        'MSE': mse_flex,
        'MAE': mae_flex,
        'MSLE': msle_flex,
        'R²': r2_flex,
        'MedAE': medae_flex,
        'MAPE': mape_flex,
        'SMAPE': smape_flex,
        'RAE': rae_flex,
        'KGE': kge_flex,
        'NSE': nse_flex,
        'EVS': evs_flex
    }

# Convert metrics to DataFrames
metrics_split_tensile_df = pd.DataFrame(metrics_split_tensile).T
metrics_flexural_df = pd.DataFrame(metrics_flexural).T

# Save results to Excel
results_file_path = 'model_1212121performance_metrics.xlsx'
with pd.ExcelWriter(results_file_path) as writer:
    metrics_split_tensile_df.to_excel(writer, sheet_name='Split Tensile Strength')
    metrics_flexural_df.to_excel(writer, sheet_name='Flexural Strength')

print(f"Metrics saved to {results_file_path}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Initialize metrics dictionaries
metrics_split_tensile = {}
metrics_flexural = {}

# Calculate performance metrics
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Metrics for Split tensile Strength
    rmse_split = np.sqrt(mean_squared_error(y_test['Split tensile Strength'], y_pred[:, 0]))
    mse_split = mean_squared_error(y_test['Split tensile Strength'], y_pred[:, 0])
    mae_split = mean_absolute_error(y_test['Split tensile Strength'], y_pred[:, 0])
    # Ensure no negative values for MSLE
    if (y_test['Split tensile Strength'] >= 0).all() and (y_pred[:, 0] >= 0).all():
        msle_split = mean_squared_log_error(y_test['Split tensile Strength'], y_pred[:, 0])
    else:
        msle_split = 'NaN'
    r2_split = r2_score(y_test['Split tensile Strength'], y_pred[:, 0])
    medae_split = median_absolute_error(y_test['Split tensile Strength'], y_pred[:, 0])
    mape_split = np.mean(np.abs((y_test['Split tensile Strength'] - y_pred[:, 0]) / y_test['Split tensile Strength'])) * 100
    smape_split = smape(y_test['Split tensile Strength'], y_pred[:, 0])
    rae_split = rae(y_test['Split tensile Strength'], y_pred[:, 0])
    kge_split = kge(y_test['Split tensile Strength'], y_pred[:, 0])
    nse_split = nse(y_test['Split tensile Strength'], y_pred[:, 0])
    evs_split = explained_variance_score(y_test['Split tensile Strength'], y_pred[:, 0])

    metrics_split_tensile[name] = {
        'RMSE': rmse_split,
        'MSE': mse_split,
        'MAE': mae_split,
        'MSLE': msle_split,
        'R²': r2_split,
        'MedAE': medae_split,
        'MAPE': mape_split,
        'SMAPE': smape_split,
        'RAE': rae_split,
        'KGE': kge_split,
        'NSE': nse_split,
        'EVS': evs_split
    }

    # Metrics for Flexural Strength
    rmse_flex = np.sqrt(mean_squared_error(y_test['Flex'], y_pred[:, 1]))
    mse_flex = mean_squared_error(y_test['Flex'], y_pred[:, 1])
    mae_flex = mean_absolute_error(y_test['Flex'], y_pred[:, 1])
    # Ensure no negative values for MSLE
    if (y_test['Flex'] >= 0).all() and (y_pred[:, 1] >= 0).all():
        msle_flex = mean_squared_log_error(y_test['Flex'], y_pred[:, 1])
    else:
        msle_flex = 'NaN'
    r2_flex = r2_score(y_test['Flex'], y_pred[:, 1])
    medae_flex = median_absolute_error(y_test['Flex'], y_pred[:, 1])
    mape_flex = np.mean(np.abs((y_test['Flex'] - y_pred[:, 1]) / y_test['Flex'])) * 100
    smape_flex = smape(y_test['Flex'], y_pred[:, 1])
    rae_flex = rae(y_test['Flex'], y_pred[:, 1])
    kge_flex = kge(y_test['Flex'], y_pred[:, 1])
    nse_flex = nse(y_test['Flex'], y_pred[:, 1])
    evs_flex = explained_variance_score(y_test['Flex'], y_pred[:, 1])

    metrics_flexural[name] = {
        'RMSE': rmse_flex,
        'MSE': mse_flex,
        'MAE': mae_flex,
        'MSLE': msle_flex,
        'R²': r2_flex,
        'MedAE': medae_flex,
        'MAPE': mape_flex,
        'SMAPE': smape_flex,
        'RAE': rae_flex,
        'KGE': kge_flex,
        'NSE': nse_flex,
        'EVS': evs_flex
    }

# Convert metrics to DataFrames
metrics_split_tensile_df = pd.DataFrame(metrics_split_tensile).T
metrics_flexural_df = pd.DataFrame(metrics_flexural).T

# Save results to Excel
results_file_path = 'model_1212121performance_metrics.xlsx'
with pd.ExcelWriter(results_file_path) as writer:
    metrics_split_tensile_df.to_excel(writer, sheet_name='Split Tensile Strength')
    metrics_flexural_df.to_excel(writer, sheet_name='Flexural Strength')

print(f"Metrics saved to {results_file_path}")

from sklearn.linear_model import HuberRegressor

pip install matplotlib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

def calculate_rec(y_true, y_pred, threshold=1.0):
    residuals = np.abs(y_true - y_pred)
    return np.sum(residuals > threshold)

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Create a directory to save plots
import os
plot_dir = 'rec_curves'
os.makedirs(plot_dir, exist_ok=True)

# Generate and save REC curves
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # REC for Split tensile Strength
    residuals_split = np.abs(y_test['Split tensile Strength'] - y_pred[:, 0])
    thresholds_split = np.linspace(0, np.max(residuals_split), num=100)
    rec_split = [np.sum(residuals_split > t) for t in thresholds_split]

    # REC for Flexural Strength
    residuals_flex = np.abs(y_test['Flex'] - y_pred[:, 1])
    thresholds_flex = np.linspace(0, np.max(residuals_flex), num=100)
    rec_flex = [np.sum(residuals_flex > t) for t in thresholds_flex]

    # Plot REC curves
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(thresholds_split, rec_split, label=f'{name} - Split Tensile Strength')
    plt.xlabel('Residual Threshold')
    plt.ylabel('Number of Predictions Exceeding Threshold')
    plt.title('REC Curve for Split Tensile Strength')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(thresholds_flex, rec_flex, label=f'{name} - Flexural Strength')
    plt.xlabel('Residual Threshold')
    plt.ylabel('Number of Predictions Exceeding Threshold')
    plt.title('REC Curve for Flexural Strength')
    plt.legend()
    plt.grid(True)

    # Save the plot
    plt.savefig(os.path.join(plot_dir, f'{name}_rec_curve.png'))
    plt.close()

print(f"REC curves saved to {plot_dir}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Initialize dictionaries to hold REC data
rec_split_tensile = {name: {'thresholds': [], 'rec': []} for name in selected_models}
rec_flexural = {name: {'thresholds': [], 'rec': []} for name in selected_models}

# Calculate REC curves for all models
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # REC for Split tensile Strength
    residuals_split = np.abs(y_test['Split tensile Strength'] - y_pred[:, 0])
    thresholds_split = np.linspace(0, np.max(residuals_split), num=100)
    rec_split = [np.sum(residuals_split > t) for t in thresholds_split]

    # REC for Flexural Strength
    residuals_flex = np.abs(y_test['Flex'] - y_pred[:, 1])
    thresholds_flex = np.linspace(0, np.max(residuals_flex), num=100)
    rec_flex = [np.sum(residuals_flex > t) for t in thresholds_flex]

    rec_split_tensile[name]['thresholds'] = thresholds_split
    rec_split_tensile[name]['rec'] = rec_split

    rec_flexural[name]['thresholds'] = thresholds_flex
    rec_flexural[name]['rec'] = rec_flex

# Plot REC curves for Split Tensile Strength
plt.figure(figsize=(12, 8))
for name, data in rec_split_tensile.items():
    plt.plot(data['thresholds'], data['rec'], label=f'{name}')
plt.xlabel('Residual Threshold', fontsize=16)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=16)
plt.title('REC Curve for Split Tensile Strength')
plt.legend()
plt.grid(True)
plt.savefig('rec_curve_split_tensile_strength.png')
plt.show()

# Plot REC curves for Flexural Strength
plt.figure(figsize=(12, 8))
for name, data in rec_flexural.items():
    plt.plot(data['thresholds'], data['rec'], label=f'{name}')
plt.xlabel('Residual Threshold', fontsize=16)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=16)
plt.title('REC Curve for Flexural Strength')
plt.legend()
plt.grid(True)
plt.savefig('rec_curve_flexural_strength.png')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Initialize dictionaries to hold REC data
rec_split_tensile = {name: {'thresholds': [], 'rec': []} for name in selected_models}
rec_flexural = {name: {'thresholds': [], 'rec': []} for name in selected_models}

# Markers for different models
markers = {
    'ElasticNet': 'o',
    'Ridge': 's',
    'AdaBoost': '^',
    'Huber': 'D',
    'Extra Trees': 'v',
    'LightGBM': '*'
}

# Calculate REC curves for all models
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # REC for Split tensile Strength
    residuals_split = np.abs(y_test['Split tensile Strength'] - y_pred[:, 0])
    thresholds_split = np.linspace(0, np.max(residuals_split), num=100)
    rec_split = [np.sum(residuals_split > t) for t in thresholds_split]

    # REC for Flexural Strength
    residuals_flex = np.abs(y_test['Flex'] - y_pred[:, 1])
    thresholds_flex = np.linspace(0, np.max(residuals_flex), num=100)
    rec_flex = [np.sum(residuals_flex > t) for t in thresholds_flex]

    rec_split_tensile[name]['thresholds'] = thresholds_split
    rec_split_tensile[name]['rec'] = rec_split

    rec_flexural[name]['thresholds'] = thresholds_flex
    rec_flexural[name]['rec'] = rec_flex

# Plot REC curves for Split Tensile Strength
plt.figure(figsize=(14, 6))
for name, data in rec_split_tensile.items():
    plt.plot(data['thresholds'], data['rec'], marker=markers[name], label=name)
plt.xlabel('Residual Threshold', fontsize=16)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=16)
plt.title('REC Curve for Split Tensile Strength')
plt.legend(fontsize=12)
plt.grid(True)
plt.savefig('rec_curve_split_tensile_strength.png')
plt.show()

# Plot REC curves for Flexural Strength
plt.figure(figsize=(14, 6))
for name, data in rec_flexural.items():
    plt.plot(data['thresholds'], data['rec'], marker=markers[name], label=name)
plt.xlabel('Residual Threshold', fontsize=16)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=16)
plt.title('REC Curve for Flexural Strength')
plt.legend(fontsize=12)
plt.grid(True)
plt.savefig('rec_curve_flexural_strength.png')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Initialize dictionaries to hold REC data
rec_split_tensile = {name: {'thresholds': [], 'rec': []} for name in selected_models}
rec_flexural = {name: {'thresholds': [], 'rec': []} for name in selected_models}

# Markers for different models
markers = {
    'ElasticNet': 'o',
    'Ridge': 's',
    'AdaBoost': '^',
    'Huber': 'D',
    'Extra Trees': 'v',
    'LightGBM': '*'
}

# Calculate REC curves for all models
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # REC for Split tensile Strength
    residuals_split = np.abs(y_test['Split tensile Strength'] - y_pred[:, 0])
    thresholds_split = np.linspace(0, np.max(residuals_split), num=100)
    rec_split = [np.sum(residuals_split > t) for t in thresholds_split]

    # REC for Flexural Strength
    residuals_flex = np.abs(y_test['Flex'] - y_pred[:, 1])
    thresholds_flex = np.linspace(0, np.max(residuals_flex), num=100)
    rec_flex = [np.sum(residuals_flex > t) for t in thresholds_flex]

    rec_split_tensile[name]['thresholds'] = thresholds_split
    rec_split_tensile[name]['rec'] = rec_split

    rec_flexural[name]['thresholds'] = thresholds_flex
    rec_flexural[name]['rec'] = rec_flex

# Plot REC curves for Split Tensile Strength
plt.figure(figsize=(12, 8))
for name, data in rec_split_tensile.items():
    plt.plot(data['thresholds'], data['rec'], marker=markers[name], label=name)
plt.xlabel('Residual Threshold', fontsize=14)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=14)
plt.title('REC Curve for Split Tensile Strength', fontsize=14)
plt.legend(fontsize=14)
plt.grid(True)
plt.savefig('rec_curve_split_tensile_strength.png')
plt.show()

# Plot REC curves for Flexural Strength
plt.figure(figsize=(12, 8))
for name, data in rec_flexural.items():
    plt.plot(data['thresholds'], data['rec'], marker=markers[name], label=name)
plt.xlabel('Residual Threshold', fontsize=14)
plt.ylabel('Number of Predictions Exceeding Threshold', fontsize=14)
plt.title('REC Curve for Flexural Strength', fontsize=14)
plt.legend(fontsize=14)
plt.grid(True)
plt.savefig('rec_curve_flexural_strength.png')
plt.show()

Ok Code till here



pip install shap

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Standardize the input features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Initialize dictionaries to hold SHAP values
shap_values = {name: {'Split tensile Strength': [], 'Flex': []} for name in selected_models}

# Calculate SHAP values for all models
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Create SHAP explainer based on the model
    if name == 'LightGBM':
        explainer = shap.TreeExplainer(model.estimators_[0])  # For LightGBM, use the tree explainer
    elif name in ['ElasticNet', 'Ridge', 'Huber']:
        explainer = shap.Explainer(model.estimators_[0])  # For linear models, use the general explainer
    elif name == 'Extra Trees':
        explainer = shap.TreeExplainer(model.estimators_[0])  # For Extra Trees, use the tree explainer
    elif name == 'AdaBoost':
        # AdaBoostRegressor does not have a built-in SHAP explainer; it needs special handling
        # We will skip AdaBoost for SHAP calculation or use another method
        continue

    # Calculate SHAP values
    shap_values_model = explainer(X_test)

    # Store SHAP values for Split tensile Strength and Flex
    shap_values[name]['Split tensile Strength'] = shap_values_model[:, 0].values
    shap_values[name]['Flex'] = shap_values_model[:, 1].values

# Plot SHAP values for Split Tensile Strength
plt.figure(figsize=(12, 8))
for name, data in shap_values.items():
    if 'Split tensile Strength' in data:
        shap.summary_plot(data['Split tensile Strength'], X_test, feature_names=X.columns, show=False)
        plt.title(f'SHAP Values for Split Tensile Strength - {name}', fontsize=14)
        plt.savefig(f'shap_split_tensile_strength_{name}.png')
        plt.show()

# Plot SHAP values for Flexural Strength
plt.figure(figsize=(12, 8))
for name, data in shap_values.items():
    if 'Flex' in data:
        shap.summary_plot(data['Flex'], X_test, feature_names=X.columns, show=False)
        plt.title(f'SHAP Values for Flexural Strength - {name}', fontsize=14)
        plt.savefig(f'shap_flexural_strength_{name}.png')
        plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import shap
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler

# Custom metrics
def smape(y_true, y_pred):
    return 100 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Standardize the input features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
selected_models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in selected_models.items()}

# Initialize dictionaries to hold SHAP values
shap_values = {name: {'Split tensile Strength': [], 'Flex': []} for name in selected_models}

# Calculate SHAP values for all models
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Create SHAP explainer based on the model
    if name == 'LightGBM':
        explainer = shap.TreeExplainer(model.estimators_[0])  # For LightGBM, use the tree explainer
    elif name in ['Extra Trees']:
        explainer = shap.TreeExplainer(model.estimators_[0])  # For Extra Trees, use the tree explainer
    elif name in ['ElasticNet', 'Ridge', 'Huber']:
        explainer = shap.LinearExplainer(model.estimators_[0], X_train)  # For linear models, use the linear explainer
    else:
        continue

    # Calculate SHAP values
    shap_values_model = explainer.shap_values(X_test)

    # Convert to DataFrame for easier handling
    shap_values_df = pd.DataFrame(shap_values_model, columns=X.columns)

    # Store SHAP values for Split tensile Strength and Flex
    shap_values[name]['Split tensile Strength'] = shap_values_df.iloc[:, :len(X.columns)].values
    shap_values[name]['Flex'] = shap_values_df.iloc[:, len(X.columns):].values

# Plot SHAP values for Split Tensile Strength
plt.figure(figsize=(12, 8))
for name, data in shap_values.items():
    if 'Split tensile Strength' in data:
        shap.summary_plot(data['Split tensile Strength'], X_test, feature_names=X.columns, show=False)
        plt.title(f'SHAP Values for Split Tensile Strength - {name}', fontsize=14)
        plt.savefig(f'shap_split_tensile_strength_{name}.png')
        plt.show()

# Plot SHAP values for Flexural Strength
plt.figure(figsize=(12, 8))
for name, data in shap_values.items():
    if 'Flex' in data:
        shap.summary_plot(data['Flex'], X_test, feature_names=X.columns, show=False)
        plt.title(f'SHAP Values for Flexural Strength - {name}', fontsize=14)
        plt.savefig(f'shap_flexural_strength_{name}.png')
        plt.show()

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import train_test_split

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Extra Trees model for each output parameter
model_split_tensile = ExtraTreesRegressor(n_estimators=100, random_state=42)
model_flex = ExtraTreesRegressor(n_estimators=100, random_state=42)

model_split_tensile.fit(X_train, y_train['Split tensile Strength'])
model_flex.fit(X_train, y_train['Flex'])

# Explain the model predictions using SHAP
explainer_split_tensile = shap.Explainer(model_split_tensile, X_train)
shap_values_split_tensile = explainer_split_tensile(X_test)

explainer_flex = shap.Explainer(model_flex, X_train)
shap_values_flex = explainer_flex(X_test)

# Plot SHAP values for Split Tensile Strength
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values_split_tensile, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Values for Split Tensile Strength', fontsize=14)
plt.savefig('shap_split_tensile_strength.png')
plt.show()

# Plot SHAP values for Flexural Strength
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values_flex, X_test, feature_names=X.columns, show=False)
plt.title('SHAP Values for Flexural Strength', fontsize=14)
plt.savefig('shap_flexural_strength.png')
plt.show()

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet, Ridge
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import HuberRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'LightGBM': LGBMRegressor()
}

# Train and plot SHAP values for each model
for name, model in models.items():
    if name == 'LightGBM':
        # LightGBM needs special handling
        model_split_tensile = model
        model_flex = model
        model_split_tensile.fit(X_train, y_train['Split tensile Strength'])
        model_flex.fit(X_train, y_train['Flex'])

        # Explain the model predictions using SHAP
        explainer_split_tensile = shap.Explainer(model_split_tensile)
        shap_values_split_tensile = explainer_split_tensile(X_test)

        explainer_flex = shap.Explainer(model_flex)
        shap_values_flex = explainer_flex(X_test)
    else:
        # For other models, fit and explain separately
        model_split_tensile = model
        model_flex = model
        model_split_tensile.fit(X_train, y_train['Split tensile Strength'])
        model_flex.fit(X_train, y_train['Flex'])

        # Explain the model predictions using SHAP
        explainer_split_tensile = shap.Explainer(model_split_tensile, X_train)
        shap_values_split_tensile = explainer_split_tensile(X_test)

        explainer_flex = shap.Explainer(model_flex, X_train)
        shap_values_flex = explainer_flex(X_test)

    # Plot SHAP values for Split Tensile Strength
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values_split_tensile, X_test, feature_names=X.columns, show=False)
    plt.title(f'SHAP Values for Split Tensile Strength - {name}', fontsize=14)
    plt.savefig(f'shap_split_tensile_strength_{name}.png')
    plt.show()

    # Plot SHAP values for Flexural Strength
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values_flex, X_test, feature_names=X.columns, show=False)
    plt.title(f'SHAP Values for Flexural Strength - {name}', fontsize=14)
    plt.savefig(f'shap_flexural_strength_{name}.png')
    plt.show()

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'LightGBM': LGBMRegressor()
}

# Function to plot SHAP values
def plot_shap_values(model, X_train, X_test, feature_names, output_name, model_name):
    if model_name in ['ElasticNet', 'Ridge', 'Huber']:
        explainer = shap.Explainer(model, X_train)
        shap_values = explainer(X_test)
    elif model_name in ['AdaBoost', 'Extra Trees']:
        explainer = shap.KernelExplainer(model.predict, X_train)
        shap_values = explainer.shap_values(X_test)
    elif model_name == 'LightGBM':
        explainer = shap.Explainer(model)
        shap_values = explainer(X_test)

    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)
    plt.title(f'SHAP Values for {output_name} - {model_name}', fontsize=14)
    plt.savefig(f'shap_{output_name.lower().replace(" ", "_")}_{model_name}.png')
    plt.show()

# Train and plot SHAP values for each model
for name, model in models.items():
    # Fit models for Split Tensile Strength
    model_split_tensile = model
    model_split_tensile.fit(X_train, y_train['Split tensile Strength'])

    # Plot SHAP values for Split Tensile Strength
    plot_shap_values(model_split_tensile, X_train, X_test, X.columns, 'Split tensile Strength', name)

    # Fit models for Flexural Strength
    model_flex = model
    model_flex.fit(X_train, y_train['Flex'])

    # Plot SHAP values for Flexural Strength
    plot_shap_values(model_flex, X_train, X_test, X.columns, 'Flexural Strength', name)

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Function to plot SHAP values
def plot_shap_values(model, X_train, X_test, feature_names, output_name, model_name):
    if model_name in ['ElasticNet', 'Ridge', 'Huber']:
        explainer = shap.Explainer(model, X_train)
        shap_values = explainer(X_test)
    elif model_name in ['AdaBoost', 'Extra Trees']:
        explainer = shap.KernelExplainer(model.predict, X_train)
        shap_values = explainer.shap_values(X_test)
    elif model_name == 'LightGBM':
        explainer = shap.Explainer(model)
        shap_values = explainer(X_test)

    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)
    plt.title(f'SHAP Values for {output_name} - {model_name}', fontsize=14)
    plt.savefig(f'shap_{output_name.lower().replace(" ", "_")}_{model_name}.png')
    plt.show()

# Train and plot SHAP values for each model
for name, model in models.items():
    # Fit models for Split Tensile Strength
    model_split_tensile = model
    model_split_tensile.fit(X_train, y_train['Split tensile Strength'])

    # Plot SHAP values for Split Tensile Strength
    plot_shap_values(model_split_tensile, X_train, X_test, X.columns, 'Split tensile Strength', name)

    # Fit models for Flexural Strength
    model_flex = model
    model_flex.fit(X_train, y_train['Flex'])

    # Plot SHAP values for Flexural Strength
    plot_shap_values(model_flex, X_train, X_test, X.columns, 'Flexural Strength', name)

import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.model_selection import train_test_split
from lightgbm import LGBMRegressor

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Function to calculate and plot SHAP values
def plot_shap_values(model, X_train, X_test, feature_names, output_name):
    explainer = shap.Explainer(model, X_train)
    shap_values = explainer(X_test)

    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)
    plt.title(f'SHAP Values for {output_name}', fontsize=14)
    plt.savefig(f'shap_{output_name.lower().replace(" ", "_")}.png')
    plt.show()

    # Calculate feature importance
    feature_importance = np.abs(shap_values.values).mean(axis=0)
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values(by='importance', ascending=False)
    return feature_importance_df

# Function to train model and get feature importance
def get_feature_importance(model, X_train, X_test, y_train, output_name):
    model.fit(X_train, y_train)
    feature_importance_df = plot_shap_values(model, X_train, X_test, X.columns, output_name)
    return feature_importance_df

# Dictionary to store feature importance dataframes
feature_importance_dict = {
    'Split tensile Strength': {},
    'Flexural Strength': {}
}

# Get feature importance for each model
for name, model in models.items():
    # Split tensile Strength
    feature_importance_dict['Split tensile Strength'][name] = get_feature_importance(
        model, X_train, X_test, y_train['Split tensile Strength'], 'Split tensile Strength'
    )
    # Flexural Strength
    feature_importance_dict['Flexural Strength'][name] = get_feature_importance(
        model, X_train, X_test, y_train['Flex'], 'Flexural Strength'
    )

# Rank features for each model
for output in feature_importance_dict:
    print(f"Feature importance ranking for {output}:")
    for model_name, importance_df in feature_importance_dict[output].items():
        print(f"\nModel: {model_name}")
        print(importance_df)

# Save feature importance rankings to CSV
for output, models_dict in feature_importance_dict.items():
    for model_name, importance_df in models_dict.items():
        importance_df.to_csv(f'feature_importance_{output.lower().replace(" ", "_")}_{model_name}.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
import joblib

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X, y_split_tensile, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X, y_flexural, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Train and save models for Split Tensile Strength
for name, model in models.items():
    model.fit(X_train_split, y_train_split)
    joblib_file = f"model_split_tensile_{name}.joblib"
    joblib.dump(model, joblib_file)
    print(f"Saved {name} model for Split Tensile Strength as {joblib_file}")

# Train and save models for Flexural Strength
for name, model in models.items():
    model.fit(X_train_flex, y_train_flex)
    joblib_file = f"model_flexural_{name}.joblib"
    joblib.dump(model, joblib_file)
    print(f"Saved {name} model for Flexural Strength as {joblib_file}")

pip install SALib

import pandas as pd
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
import joblib
from SALib.sample import saltelli
from SALib.analyze import sobol
import numpy as np

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X, y_split_tensile, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X, y_flexural, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Define the problem for SALib
problem = {
    'num_vars': X.shape[1],
    'names': X.columns.tolist(),
    'bounds': [[X[col].min(), X[col].max()] for col in X.columns]
}

# Function to evaluate the model for Sobol analysis
def evaluate_model(model, X):
    return model.predict(X)

# Perform global sensitivity analysis and print results for each model
for name, model in models.items():
    print(f"Performing sensitivity analysis for {name} model")

    # Train the model for Split Tensile Strength
    model.fit(X_train_split, y_train_split)

    # Generate samples
    param_values = saltelli.sample(problem, 1000)

    # Evaluate the model
    Y = evaluate_model(model, param_values)

    # Perform Sobol sensitivity analysis
    Si = sobol.analyze(problem, Y)

    # Print Sobol sensitivity indices
    print(f"Sobol sensitivity indices for Split Tensile Strength - {name}:")
    print("First order sensitivity indices:")
    print(Si['S1'])
    print("Total order sensitivity indices:")
    print(Si['ST'])
    print()

    # Train the model for Flexural Strength
    model.fit(X_train_flex, y_train_flex)

    # Evaluate the model
    Y = evaluate_model(model, param_values)

    # Perform Sobol sensitivity analysis
    Si = sobol.analyze(problem, Y)

    # Print Sobol sensitivity indices
    print(f"Sobol sensitivity indices for Flexural Strength - {name}:")
    print("First order sensitivity indices:")
    print(Si['S1'])
    print("Total order sensitivity indices:")
    print(Si['ST'])
    print()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from SALib.sample import saltelli
from SALib.analyze import sobol
import numpy as np

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X, y_split_tensile, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X, y_flexural, test_size=0.2, random_state=42)

# Initialize models
models = {
    'ElasticNet': ElasticNet(),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Define the problem for SALib
problem = {
    'num_vars': X.shape[1],
    'names': X.columns.tolist(),
    'bounds': [[X[col].min(), X[col].max()] for col in X.columns]
}

# Function to evaluate the model for Sobol analysis
def evaluate_model(model, X):
    return model.predict(X)

# Perform global sensitivity analysis and plot results for each model
for name, model in models.items():
    print(f"Performing sensitivity analysis for {name} model")

    # Train the model for Split Tensile Strength
    model.fit(X_train_split, y_train_split)

    # Generate samples
    param_values = saltelli.sample(problem, 1000)

    # Evaluate the model
    Y = evaluate_model(model, param_values)

    # Perform Sobol sensitivity analysis
    Si = sobol.analyze(problem, Y)

    # Plot Sobol sensitivity indices for Split Tensile Strength
    plt.figure(figsize=(12, 8))
    plt.subplot(1, 2, 1)
    plt.bar(problem['names'], Si['S1'], color='skyblue')
    plt.title(f'First Order Sensitivity (Split Tensile Strength) - {name}', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=10)
    plt.ylabel('Sensitivity Index', fontsize=14)
    plt.yticks(fontsize=10)
    plt.subplot(1, 2, 2)
    plt.bar(problem['names'], Si['ST'], color='lightcoral')
    plt.title(f'Total Order Sensitivity (Split Tensile Strength) - {name}', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=10)
    plt.ylabel('Sensitivity Index', fontsize=14)
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.savefig(f'sensitivity_split_tensile_{name}.png')
    plt.show()

    # Train the model for Flexural Strength
    model.fit(X_train_flex, y_train_flex)

    # Evaluate the model
    Y = evaluate_model(model, param_values)

    # Perform Sobol sensitivity analysis
    Si = sobol.analyze(problem, Y)

    # Plot Sobol sensitivity indices for Flexural Strength
    plt.figure(figsize=(12, 8))
    plt.subplot(1, 2, 1)
    plt.bar(problem['names'], Si['S1'], color='skyblue')
    plt.title(f'First Order Sensitivity (Flexural Strength) - {name}', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=10)
    plt.ylabel('Sensitivity Index', fontsize=14)
    plt.yticks(fontsize=10)
    plt.subplot(1, 2, 2)
    plt.bar(problem['names'], Si['ST'], color='lightcoral')
    plt.title(f'Total Order Sensitivity (Flexural Strength) - {name}', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=10)
    plt.ylabel('Sensitivity Index', fontsize=14)
    plt.yticks(fontsize=10)
    plt.tight_layout()
    plt.savefig(f'sensitivity_flexural_{name}.png')
    plt.show()

pip install --upgrade scikit-learn

import sklearn
print(sklearn.__version__)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import train_test_split
from sklearn.inspection import PartialDependenceDisplay

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Sample a subset of the data to manage memory usage
X_subset = X.sample(frac=0.1, random_state=42)
y_split_tensile_subset = y_split_tensile.loc[X_subset.index]
y_flexural_subset = y_flexural.loc[X_subset.index]

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_subset, y_split_tensile_subset, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X_subset, y_flexural_subset, test_size=0.2, random_state=42)

# Initialize and train the ElasticNet model
elastic_net = ElasticNet()
elastic_net.fit(X_train_split, y_train_split)

# Perform Partial Dependence Analysis for Split Tensile Strength
fig, ax = plt.subplots(figsize=(12, 8))
pd_display_split_tensile = PartialDependenceDisplay.from_estimator(
    elastic_net, X_train_split,
    features=[0, 1, 2, 3, 4, 5, 6],  # Features to analyze
    feature_names=X_subset.columns,
    ax=ax
)

ax.set_xlabel('Feature Value', fontsize=10)
ax.set_ylabel('Partial Dependence', fontsize=10)
ax.set_title('Partial Dependence - Split Tensile Strength (ElasticNet)', fontsize=14)
plt.savefig('partial_dependence_split_tensile_elastic_net.png')
plt.show()

# Perform Partial Dependence Analysis for Flexural Strength
fig, ax = plt.subplots(figsize=(12, 8))
pd_display_flexural = PartialDependenceDisplay.from_estimator(
    elastic_net, X_train_flex,
    features=[0, 1, 2, 3, 4, 5, 6],  # Features to analyze
    feature_names=X_subset.columns,
    ax=ax
)

ax.set_xlabel('Feature Value', fontsize=10)
ax.set_ylabel('Partial Dependence', fontsize=10)
ax.set_title('Partial Dependence - Flexural Strength (ElasticNet)', fontsize=14)
plt.savefig('partial_dependence_flexural_elastic_net.png')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import train_test_split
from sklearn.inspection import PartialDependenceDisplay

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Sample a subset of the data to manage memory usage
X_subset = X.sample(frac=0.1, random_state=42)
y_split_tensile_subset = y_split_tensile.loc[X_subset.index]
y_flexural_subset = y_flexural.loc[X_subset.index]

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_subset, y_split_tensile_subset, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X_subset, y_flexural_subset, test_size=0.2, random_state=42)

# Initialize and train the ElasticNet model for Split Tensile Strength
elastic_net_split = ElasticNet(max_iter=10000)  # Increased max_iter to avoid convergence issues
elastic_net_split.fit(X_train_split, y_train_split)

# Perform Partial Dependence Analysis for Split Tensile Strength
fig, ax = plt.subplots(figsize=(12, 8))
pd_display_split_tensile = PartialDependenceDisplay.from_estimator(
    elastic_net_split, X_train_split,
    features=[0, 1, 2, 3, 4, 5, 6],  # Features to analyze
    feature_names=X_subset.columns,
    ax=ax
)

ax.set_xlabel('Feature Value', fontsize=10)
ax.set_ylabel('Partial Dependence', fontsize=10)
ax.set_title('Partial Dependence - Split Tensile Strength (ElasticNet)', fontsize=14)
plt.tight_layout()
plt.savefig('partial_dependence_split_tensile_elastic_net.png')
plt.show()

# Initialize and train the ElasticNet model for Flexural Strength
elastic_net_flex = ElasticNet(max_iter=10000)  # Increased max_iter to avoid convergence issues
elastic_net_flex.fit(X_train_flex, y_train_flex)

# Perform Partial Dependence Analysis for Flexural Strength
fig, ax = plt.subplots(figsize=(12, 8))
pd_display_flexural = PartialDependenceDisplay.from_estimator(
    elastic_net_flex, X_train_flex,
    features=[0, 1, 2, 3, 4, 5, 6],  # Features to analyze
    feature_names=X_subset.columns,
    ax=ax
)

ax.set_xlabel('Feature Value', fontsize=10)
ax.set_ylabel('Partial Dependence', fontsize=10)
ax.set_title('Partial Dependence - Flexural Strength (ElasticNet)', fontsize=14)
plt.tight_layout()
plt.savefig('partial_dependence_flexural_elastic_net.png')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.inspection import PartialDependenceDisplay

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Sample a subset of the data to manage memory usage
X_subset = X.sample(frac=0.1, random_state=42)
y_split_tensile_subset = y_split_tensile.loc[X_subset.index]
y_flexural_subset = y_flexural.loc[X_subset.index]

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_subset, y_split_tensile_subset, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X_subset, y_flexural_subset, test_size=0.2, random_state=42)

# Initialize the models
models = {
    'ElasticNet': ElasticNet(max_iter=10000),
    'Ridge': Ridge(),
    'AdaBoost': AdaBoostRegressor(),
    'Huber': HuberRegressor(max_iter=2000),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Function to train the model and plot partial dependence
def plot_partial_dependence(model_name, model, X_train, y_train, output_name):
    model.fit(X_train, y_train)

    fig, ax = plt.subplots(figsize=(12, 8))
    pd_display = PartialDependenceDisplay.from_estimator(
        model, X_train,
        features=[0, 1, 2, 3, 4, 5, 6],  # Features to analyze
        feature_names=X_subset.columns,
        ax=ax
    )
    ax.set_title(f'Partial Dependence - {output_name} ({model_name})', fontsize=16)
    ax.set_xlabel('Feature Value', fontsize=14)
    ax.set_ylabel('Partial Dependence', fontsize=14)
    ax.tick_params(axis='both', which='major', labelsize=12)

    plt.tight_layout()
    plt.savefig(f'partial_dependence_{output_name.lower().replace(" ", "_")}_{model_name.lower()}.png')
    plt.show()

# Plot for Split Tensile Strength
for model_name, model in models.items():
    plot_partial_dependence(model_name, model, X_train_split, y_train_split, 'Split Tensile Strength')

# Plot for Flexural Strength
for model_name, model in models.items():
    plot_partial_dependence(model_name, model, X_train_flex, y_train_flex, 'Flexural Strength')

pip install pyswarm

import pandas as pd
from sklearn.linear_model import ElasticNet, Ridge, HuberRegressor
from sklearn.ensemble import AdaBoostRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Load the dataset
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y_split_tensile = df['Split tensile Strength']
y_flexural = df['Flex']

# Sample a subset of the data to manage memory usage
X_subset = X.sample(frac=0.1, random_state=42)
y_split_tensile_subset = y_split_tensile.loc[X_subset.index]
y_flexural_subset = y_flexural.loc[X_subset.index]

# Split the data into training and test sets for both target variables
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_subset, y_split_tensile_subset, test_size=0.2, random_state=42)
X_train_flex, X_test_flex, y_train_flex, y_test_flex = train_test_split(X_subset, y_flexural_subset, test_size=0.2, random_state=42)

# Initialize the models
models = {
    'ElasticNet': ElasticNet(max_iter=10000),
    'Ridge': Ridge(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

def print_r2_scores(model_name, model, X_train, X_test, y_train, y_test, target_name):
    # Fit the model
    model.fit(X_train, y_train)

    # Predict on the test set
    predictions = model.predict(X_test)

    # Calculate R² score
    r2 = r2_score(y_test, predictions)

    print(f'R² for {target_name} ({model_name}): {r2:.4f}')

# Print R² scores for Split Tensile Strength
print('\n--- R² Scores for Split Tensile Strength ---')
for model_name, model in models.items():
    print_r2_scores(model_name, model, X_train_split, X_test_split, y_train_split, y_test_split, 'Split Tensile Strength')

# Print R² scores for Flexural Strength
print('\n--- R² Scores for Flexural Strength ---')
for model_name, model in models.items():
    print_r2_scores(model_name, model, X_train_flex, X_test_flex, y_train_flex, y_test_flex, 'Flexural Strength')

"""import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, mean_squared_log_error, explained_variance_score
import openpyxl
from openpyxl import Workbook

# Custom metrics
def smape(y_true, y_pred):
    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))

def rae(y_true, y_pred):
    return np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(y_true - np.mean(y_true, axis=0)))

def kge(y_true, y_pred):
    r = np.corrcoef(y_true.T, y_pred.T)[0, 1]
    alpha = np.std(y_pred, axis=0) / np.std(y_true, axis=0)
    beta = np.mean(y_pred, axis=0) / np.mean(y_true, axis=0)
    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)

def nse(y_true, y_pred):
    return 1 - (np.sum((y_true - y_pred)**2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0))**2, axis=0))

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate performance metrics
metrics = {}
for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))
    mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')
    mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')
    # Ensure no negative values for MSLE
    if (y_test >= 0).all().all() and (y_pred >= 0).all().all():
        msle = mean_squared_log_error(y_test, y_pred, multioutput='raw_values')
    else:
        msle = [np.nan, np.nan]
    r2 = r2_score(y_test, y_pred, multioutput='raw_values')
    medae = median_absolute_error(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
    smape_val = smape(y_test, y_pred)
    rae_val = rae(y_test, y_pred)
    kge_val = kge(y_test, y_pred)
    nse_val = nse(y_test, y_pred)
    evs = explained_variance_score(y_test, y_pred, multioutput='raw_values')

    metrics[name] = {
        'RMSE': rmse,
        'MSE': mse,
        'MAE': mae,
        'MSLE': msle,
        'R²': r2,
        'MedAE': medae,
        'MAPE': mape,
        'SMAPE': smape_val,
        'RAE': rae_val,
        'KGE': kge_val,
        'NSE': nse_val,
        'EVS': evs
    }

# Save results to Excel
# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name] + list(actual) + list(predicted))

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for key in ['RMSE', 'MSE', 'MAE', 'MSLE', 'R²', 'MedAE', 'MAPE', 'SMAPE', 'RAE', 'KGE', 'NSE', 'EVS']:
        if key in metric_values:
            metric = metric_values[key]
            if isinstance(metric, np.ndarray):
                metric = metric.tolist()
            if isinstance(metric, list) and len(metric) == 2:
                row.extend([float(v) for v in metric])
            else:
                row.extend([float(metric)] * 2)  # Assuming single value metric for both outputs
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

"""

import openpyxl
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for metric in metric_header[1:]:  # Skip the first 'Model' column
        split_metric, flex_metric = metric_values[metric].flatten()
        row.append(split_metric)
        row.append(flex_metric)
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

pip install openpyxl

import openpyxl
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for metric in metric_header[1:]:  # Skip the first 'Model' column
        split_metric, flex_metric = metric_values[metric].flatten()
        row.append(split_metric)
        row.append(flex_metric)
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

import openpyxl
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Create a dictionary to map error metric names to their abbreviations in the metrics dictionary
metric_abbr = {
    'RMSE': 'rmse', 'MSE': 'mse', 'MAE': 'mae', 'MSLE': 'msle', 'R²': 'r2', 'MedAE': 'medae',
    'MAPE': 'mape', 'SMAPE': 'smape', 'RAE': 'rae', 'KGE': 'kge', 'NSE': 'nse', 'EVS': 'evs'
}

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for metric in metric_header[1:]:  # Skip the first 'Model' column
        # Split metric name into metric type and parameter
        metric_type, param = metric.split()
        metric_key = metric_abbr[metric_type]
        # Append split and flex metrics
        row.append(metric_values[metric_key][0])
        row.append(metric_values[metric_key][1])
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

pip install openpyxl

import openpyxl
from openpyxl import Workbook

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    row.append(metric_values['RMSE'][0])
    row.append(metric_values['RMSE'][1])
    row.append(metric_values['MSE'][0])
    row.append(metric_values['MSE'][1])
    row.append(metric_values['MAE'][0])
    row.append(metric_values['MAE'][1])
    row.append(metric_values['MSLE'][0])
    row.append(metric_values['MSLE'][1])
    row.append(metric_values['R²'][0])
    row.append(metric_values['R²'][1])
    row.append(metric_values['MedAE'][0])
    row.append(metric_values['MedAE'][1])
    row.append(metric_values['MAPE'][0])
    row.append(metric_values['MAPE'][1])
    row.append(metric_values['SMAPE'][0])
    row.append(metric_values['SMAPE'][1])
    row.append(metric_values['RAE'][0])
    row.append(metric_values['RAE'][1])
    row.append(metric_values['KGE'][0])
    row.append(metric_values['KGE'][1])
    row.append(metric_values['NSE'][0])
    row.append(metric_values['NSE'][1])
    row.append(metric_values['EVS'][0])
    row.append(metric_values['EVS'][1])
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

import openpyxl
from openpyxl import Workbook
import numpy as np

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for key in ['RMSE', 'MSE', 'MAE', 'MSLE', 'R²', 'MedAE', 'MAPE', 'SMAPE', 'RAE', 'KGE', 'NSE', 'EVS']:
        metric = metric_values[key]
        # Ensure the metric is an array
        if not isinstance(metric, np.ndarray):
            metric = np.array([metric, metric])
        row.append(metric[0])
        row.append(metric[1])
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

import openpyxl
from openpyxl import Workbook
import numpy as np

# Prepare the Excel file
wb = Workbook()

# Create the first sheet for actual vs predicted values
ws1 = wb.active
ws1.title = "Actual vs Predicted"

# Write headers
header = ['Model', 'Actual Split Tensile Strength', 'Predicted Split Tensile Strength', 'Actual Flex', 'Predicted Flex']
ws1.append(header)

# Write actual vs predicted values for each model
for name, model in multi_output_models.items():
    y_pred = model.predict(X_test)
    for actual, predicted in zip(y_test.values, y_pred):
        ws1.append([name, actual[0], predicted[0], actual[1], predicted[1]])

# Create the second sheet for error metrics
ws2 = wb.create_sheet(title="Error Metrics")

# Write headers
metric_header = ['Model', 'RMSE Split', 'RMSE Flex', 'MSE Split', 'MSE Flex', 'MAE Split', 'MAE Flex',
                 'MSLE Split', 'MSLE Flex', 'R² Split', 'R² Flex', 'MedAE Split', 'MedAE Flex',
                 'MAPE Split', 'MAPE Flex', 'SMAPE Split', 'SMAPE Flex', 'RAE Split', 'RAE Flex',
                 'KGE Split', 'KGE Flex', 'NSE Split', 'NSE Flex', 'EVS Split', 'EVS Flex']
ws2.append(metric_header)

# Write error metrics for each model
for name, metric_values in metrics.items():
    row = [name]
    for key in ['RMSE', 'MSE', 'MAE', 'MSLE', 'R²', 'MedAE', 'MAPE', 'SMAPE', 'RAE', 'KGE', 'NSE', 'EVS']:
        metric = metric_values[key]
        # Ensure the metric is an array
        if not isinstance(metric, np.ndarray):
            metric = np.array([metric, metric])
        row.append(float(metric[0]))
        row.append(float(metric[1]))
    ws2.append(row)

# Save the Excel file
output_file = 'model_performance.xlsx'
wb.save(output_file)

print(f"Results saved to {output_file}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores and store predictions
r2_scores = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Save the results to an Excel file
output_file_path = '/mnt/data/model_predictions.xlsx'
results.to_excel(output_file_path, index=False)

# Display the file path
output_file_path

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Sample data creation (since we don't have access to the actual file)
data = {
    'OPC': [200, 210, 190, 180, 205],
    'NFA': [600, 610, 590, 580, 605],
    'CA': [1200, 1210, 1190, 1180, 1205],
    'RSA Content': [5, 5.5, 4.5, 4.8, 5.2],
    'SP': [1.5, 1.6, 1.4, 1.3, 1.5],
    'Water': [150, 155, 145, 148, 152],
    'Curing Days': [28, 30, 27, 26, 29],
    'Split tensile Strength': [3.2, 3.4, 3.1, 3.0, 3.3],
    'Flex': [4.2, 4.3, 4.1, 4.0, 4.3]
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores and store predictions
r2_scores = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Save the results to an Excel file
output_file_path = '/mnt/data/model_predictions.xlsx'
results.to_excel(output_file_path, index=False)

# Display the file path
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores and store predictions
r2_scores = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Save the results to an Excel file
output_file_path = '/mnt/data/model_predictions.xlsx'
results.to_excel(output_file_path, index=False)

# Display the file path
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores and store predictions
r2_scores = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Save the results to an Excel file
output_file_path = 'model_predictions.xlsx'
results.to_excel(output_file_path, index=False)

# Display the file path
output_file_path

pip install pandas numpy scikit-learn xgboost openpyxl

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score,
    median_absolute_error, explained_variance_score, max_error
)
import openpyxl

# Read the Excel file
file_path = 'Book2.xlsx'
df = pd.read_excel(file_path)
# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)
# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(max_iter=1000),
    'KNN': KNeighborsRegressor(),
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    msle = mean_squared_log_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    medae = median_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    smape = 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))
    rae = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true - np.mean(y_true)))
    kge = 1 - np.sqrt((np.corrcoef(y_true, y_pred)[0,1] - 1)**2 + ((np.std(y_pred) / np.std(y_true)) - 1)**2 + ((np.mean(y_pred) / np.mean(y_true)) - 1)**2)
    nse = 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))
    evs = explained_variance_score(y_true, y_pred)
    max_err = max_error(y_true, y_pred)

    return {
        'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MSLE': msle, 'R²': r2,
        'MedAE': medae, 'MAPE': mape, 'SMAPE': smape, 'RAE': rae, 'KGE': kge,
        'NSE': nse, 'EVS': evs, 'Max Error': max_err
    }

# Dictionary to store results
results = {}
predictions = {}

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = calculate_metrics(y_test, y_pred)
    predictions[name] = y_pred

# Write results to Excel
with pd.ExcelWriter('Model_Results.xlsx', engine='openpyxl') as writer:
    # Write actual vs predicted values
    for name, y_pred in predictions.items():
        df_results = pd.DataFrame({'Actual': y_test, f'Predicted_{name}': y_pred})
        df_results.to_excel(writer, sheet_name=f'{name}_Predictions', index=False)

    # Write metrics
    metrics_df = pd.DataFrame(results).T
    metrics_df.to_excel(writer, sheet_name='Metrics')

print("Results have been written to 'Model_Results.xlsx'.")



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate R² scores and store predictions
r2_scores = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_scores[name] = r2_score(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Save the results to an Excel file in the current working directory
output_file_path = 'model_predictions.xlsx'
results.to_excel(output_file_path, index=False)

# Display the file path
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score, explained_variance_score, max_error

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    msle = mean_squared_log_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    medae = median_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    smape = 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))
    rae = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true - np.mean(y_true)))
    kge = 1 - np.sqrt((np.corrcoef(y_true.T, y_pred.T)[0,1] - 1)**2 + ((np.std(y_pred) / np.std(y_true)) - 1)**2 + ((np.mean(y_pred) / np.mean(y_true)) - 1)**2)
    nse = 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))
    evs = explained_variance_score(y_true, y_pred)

    return {
        'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MSLE': msle, 'R²': r2,
        'MedAE': medae, 'MAPE': mape, 'SMAPE': smape, 'RAE': rae, 'KGE': kge,
        'NSE': nse, 'EVS': evs
    }

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_metrics(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_df = pd.DataFrame(metrics).T

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics')

# Display the file path
output_file_path = 'model_predictions_with_metrics.xlsx'
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score, explained_variance_score

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    metrics = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        rmse = np.sqrt(mean_squared_error(y_t, y_p))
        mse = mean_squared_error(y_t, y_p)
        mae = mean_absolute_error(y_t, y_p)
        msle = mean_squared_log_error(y_t, y_p)
        r2 = r2_score(y_t, y_p)
        medae = median_absolute_error(y_t, y_p)
        mape = np.mean(np.abs((y_t - y_p) / y_t)) * 100
        smape = 100 / len(y_t) * np.sum(2 * np.abs(y_p - y_t) / (np.abs(y_t) + np.abs(y_p)))
        rae = np.sum(np.abs(y_t - y_p)) / np.sum(np.abs(y_t - np.mean(y_t)))
        kge = 1 - np.sqrt((np.corrcoef(y_t, y_p)[0, 1] - 1) ** 2 + ((np.std(y_p) / np.std(y_t)) - 1) ** 2 + ((np.mean(y_p) / np.mean(y_t)) - 1) ** 2)
        nse = 1 - (np.sum((y_t - y_p) ** 2) / np.sum((y_t - np.mean(y_t)) ** 2))
        evs = explained_variance_score(y_t, y_p)

        metrics[col] = {
            'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MSLE': msle, 'R²': r2,
            'MedAE': medae, 'MAPE': mape, 'SMAPE': smape, 'RAE': rae, 'KGE': kge,
            'NSE': nse, 'EVS': evs
        }
    return metrics

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_metrics(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'],
    'Actual Flex': y_test['Flex']
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_df = pd.DataFrame({(model, metric): metrics[model][output][metric]
                          for model in metrics.keys()
                          for output in metrics[model].keys()
                          for metric in metrics[model][output].keys()}, index=[0]).T.reset_index()
metrics_df.columns = ['Model', 'Output', 'Metric', 'Value']

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)

# Display the file path
output_file_path = 'model_predictions_with_metrics.xlsx'
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score, explained_variance_score

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    metrics = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        rmse = np.sqrt(mean_squared_error(y_t, y_p))
        mse = mean_squared_error(y_t, y_p)
        mae = mean_absolute_error(y_t, y_p)
        msle = mean_squared_log_error(y_t, y_p)
        r2 = r2_score(y_t, y_p)
        medae = median_absolute_error(y_t, y_p)
        mape = np.mean(np.abs((y_t - y_p) / y_t)) * 100
        smape = 100 / len(y_t) * np.sum(2 * np.abs(y_p - y_t) / (np.abs(y_t) + np.abs(y_p)))
        rae = np.sum(np.abs(y_t - y_p)) / np.sum(np.abs(y_t - np.mean(y_t)))
        kge = 1 - np.sqrt((np.corrcoef(y_t, y_p)[0, 1] - 1) ** 2 + ((np.std(y_p) / np.std(y_t)) - 1) ** 2 + ((np.mean(y_p) / np.mean(y_t)) - 1) ** 2)
        nse = 1 - (np.sum((y_t - y_p) ** 2) / np.sum((y_t - np.mean(y_t)) ** 2))
        evs = explained_variance_score(y_t, y_p)

        metrics[col] = {
            'RMSE': rmse, 'MSE': mse, 'MAE': mae, 'MSLE': msle, 'R²': r2,
            'MedAE': medae, 'MAPE': mape, 'SMAPE': smape, 'RAE': rae, 'KGE': kge,
            'NSE': nse, 'EVS': evs
        }
    return metrics

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_metrics(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'].values,
    'Actual Flex': y_test['Flex'].values
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_list = []
for model, outputs in metrics.items():
    for output, metrics_dict in outputs.items():
        for metric, value in metrics_dict.items():
            metrics_list.append([model, output, metric, value])

metrics_df = pd.DataFrame(metrics_list, columns=['Model', 'Output', 'Metric', 'Value'])

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)

# Display the file path
output_file_path = 'model_predictions_with_metrics.xlsx'
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Function to calculate metrics
def calculate_metrics(y_true, y_pred):
    metrics = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        r2 = r2_score(y_t, y_p)
        metrics[col] = {'R²': r2}
    return metrics

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'Linear Regression': LinearRegression(),
    'MLP': MLPRegressor(max_iter=1000),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(n_neighbors=3),  # Set n_neighbors <= n_samples
    'XGB': XGBRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

# Multi-output wrapper
multi_output_models = {name: MultiOutputRegressor(model) for name, model in models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in multi_output_models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_metrics(y_test, y_pred)
    predictions[name] = y_pred

# Display R² values for ML models only
print("R² values for Split tensile Strength:")
for model in models.keys():
    print(f"{model}: {metrics[model]['Split tensile Strength']['R²']}")

print("\nR² values for Flexural Strength:")
for model in models.keys():
    print(f"{model}: {metrics[model]['Flex']['R²']}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.linear_model import TheilSenRegressor, QuantileRegressor
from sklearn.linear_model import OrthogonalMatchingPursuit, SGDRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, StackingRegressor, ExtraTreesRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score

# Function to calculate R2 score
def calculate_r2(y_true, y_pred):
    r2_scores = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        r2_scores[col] = r2_score(y_t, y_p)
    return r2_scores

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize new models
models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Elastic Net': ElasticNet(),
    'Huber': HuberRegressor(),
    'Theil-Sen': TheilSenRegressor(),
    'Quantile': QuantileRegressor(),
    'OMP': OrthogonalMatchingPursuit(),
    'SGD': SGDRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Stacking': StackingRegressor(estimators=[
        ('ridge', Ridge()),
        ('lasso', Lasso()),
        ('elastic', ElasticNet())
    ], final_estimator=Ridge()),
    'Extra Trees': ExtraTreesRegressor(),
    'CatBoost': CatBoostRegressor(silent=True),
    'LightGBM': LGBMRegressor()
}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_r2(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'].values,
    'Actual Flex': y_test['Flex'].values
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_list = []
for model, r2_scores in metrics.items():
    for output, r2 in r2_scores.items():
        metrics_list.append([model, output, 'R²', r2])

metrics_df = pd.DataFrame(metrics_list, columns=['Model', 'Output', 'Metric', 'Value'])

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics_new_models.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)

# Display the file path
output_file_path = 'model_predictions_with_metrics_new_models.xlsx'
output_file_path

!pip install catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.linear_model import TheilSenRegressor, QuantileRegressor
from sklearn.linear_model import OrthogonalMatchingPursuit, SGDRegressor
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, StackingRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score

# Function to calculate R2 score
def calculate_r2(y_true, y_pred):
    r2_scores = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        r2_scores[col] = r2_score(y_t, y_p)
    return r2_scores

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize new models
models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Elastic Net': ElasticNet(),
    'Huber': HuberRegressor(),
    'Theil-Sen': TheilSenRegressor(),
    'Quantile': QuantileRegressor(),
    'OMP': OrthogonalMatchingPursuit(),
    'SGD': SGDRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Stacking': StackingRegressor(estimators=[
        ('ridge', Ridge()),
        ('lasso', Lasso()),
        ('elastic', ElasticNet())
    ], final_estimator=Ridge()),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_r2(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'].values,
    'Actual Flex': y_test['Flex'].values
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_list = []
for model, r2_scores in metrics.items():
    for output, r2 in r2_scores.items():
        metrics_list.append([model, output, 'R²', r2])

metrics_df = pd.DataFrame(metrics_list, columns=['Model', 'Output', 'Metric', 'Value'])

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics_new_models.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)

# Display the file path
output_file_path = 'model_predictions_with_metrics_new_models.xlsx'
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.linear_model import TheilSenRegressor, QuantileRegressor
from sklearn.linear_model import OrthogonalMatchingPursuit, SGDRegressor
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, StackingRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Function to calculate R2 score
def calculate_r2(y_true, y_pred):
    r2_scores = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        r2_scores[col] = r2_score(y_t, y_p)
    return r2_scores

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize new models
base_models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Elastic Net': ElasticNet(),
    'Huber': HuberRegressor(),
    'Theil-Sen': TheilSenRegressor(),
    'Quantile': QuantileRegressor(),
    'OMP': OrthogonalMatchingPursuit(),
    'SGD': SGDRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper for models
models = {name: MultiOutputRegressor(model) for name, model in base_models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_r2(y_test, y_pred)
    predictions[name] = y_pred

# Create a DataFrame to store the actual and predicted values
results = pd.DataFrame({
    'Actual Split tensile Strength': y_test['Split tensile Strength'].values,
    'Actual Flex': y_test['Flex'].values
})

for name, y_pred in predictions.items():
    results[f'Predicted Split tensile Strength ({name})'] = y_pred[:, 0]
    results[f'Predicted Flex ({name})'] = y_pred[:, 1]

# Convert metrics dictionary to DataFrame for better representation
metrics_list = []
for model, r2_scores in metrics.items():
    for output, r2 in r2_scores.items():
        metrics_list.append([model, output, 'R²', r2])

metrics_df = pd.DataFrame(metrics_list, columns=['Model', 'Output', 'Metric', 'Value'])

# Save the results to an Excel file in the current working directory
with pd.ExcelWriter('model_predictions_with_metrics_new_models.xlsx') as writer:
    results.to_excel(writer, sheet_name='Predictions', index=False)
    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)

# Display the file path
output_file_path = 'model_predictions_with_metrics_new_models.xlsx'
output_file_path

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor
from sklearn.linear_model import TheilSenRegressor, QuantileRegressor
from sklearn.linear_model import OrthogonalMatchingPursuit, SGDRegressor
from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor
from lightgbm import LGBMRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import r2_score

# Function to calculate R2 score
def calculate_r2(y_true, y_pred):
    r2_scores = {}
    for i, col in enumerate(y_true.columns):
        y_t = y_true.iloc[:, i]
        y_p = y_pred[:, i]
        r2_scores[col] = r2_score(y_t, y_p)
    return r2_scores

# Create a larger synthetic dataset
np.random.seed(42)
data = {
    'OPC': np.random.randint(180, 220, 100),
    'NFA': np.random.randint(580, 620, 100),
    'CA': np.random.randint(1180, 1220, 100),
    'RSA Content': np.random.uniform(4.5, 5.5, 100),
    'SP': np.random.uniform(1.3, 1.6, 100),
    'Water': np.random.randint(145, 155, 100),
    'Curing Days': np.random.randint(26, 31, 100),
    'Split tensile Strength': np.random.uniform(3.0, 3.5, 100),
    'Flex': np.random.uniform(4.0, 4.5, 100)
}
df = pd.DataFrame(data)

# Define input and output parameters
X = df[['OPC', 'NFA', 'CA', 'RSA Content', 'SP', 'Water', 'Curing Days']]
y = df[['Split tensile Strength', 'Flex']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize new models
base_models = {
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'Elastic Net': ElasticNet(),
    'Huber': HuberRegressor(max_iter=2000),
    'Theil-Sen': TheilSenRegressor(),
    'Quantile': QuantileRegressor(solver='highs'),
    'OMP': OrthogonalMatchingPursuit(),
    'SGD': SGDRegressor(max_iter=2000),
    'AdaBoost': AdaBoostRegressor(),
    'Bagging': BaggingRegressor(),
    'Extra Trees': ExtraTreesRegressor(),
    'LightGBM': LGBMRegressor()
}

# Multi-output wrapper for models
models = {name: MultiOutputRegressor(model) for name, model in base_models.items()}

# Calculate metrics and store predictions
metrics = {}
predictions = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    metrics[name] = calculate_r2(y_test, y_pred)
    predictions[name] = y_pred

# Convert metrics dictionary to DataFrame for better representation
metrics_list = []
for model, r2_scores in metrics.items():
    for output, r2 in r2_scores.items():
        metrics_list.append([model, output, 'R²', r2])

metrics_df = pd.DataFrame(metrics_list, columns=['Model', 'Output', 'Metric', 'Value'])

# Display the R² metrics
print(metrics_df)

# Optionally, save the metrics to an Excel file
output_file_path = 'model_r2_metrics.xlsx'
metrics_df.to_excel(output_file_path, index=False)

output_file_path